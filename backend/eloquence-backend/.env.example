# Backend Configuration
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=info

# Database
# DATABASE_URL=postgresql+psycopg2://user:password@host:port/dbname
# Pour la base de donn√©es locale Docker:
POSTGRES_USER=postgres
POSTGRES_PASSWORD=changethis
POSTGRES_DB=eloquence

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# Celery
CELERY_BROKER_URL=redis://localhost:6379/1
CELERY_RESULT_BACKEND=redis://localhost:6379/2

# Service URLs
ASR_API_URL=http://localhost:9000/transcribe
TTS_API_URL=http://localhost:5002/api/tts # URL Coqui TTS Server
KALDI_DOCKER_IMAGE=kaldiasr/kaldi:latest
KALDI_CONTAINER_NAME=kaldi_container
KALDI_RECIPE_DIR=/kaldi/egs/librispeech
KALDI_LANG_DIR=data/lang
KALDI_MODEL_DIR=exp/chain/tdnn_1d_sp
KALDI_ALIGN_SCRIPT=steps/nnet3/align.sh
KALDI_GOP_SCRIPT=steps/compute_gop.sh

# LLM Configuration
# Choose your LLM backend by setting the appropriate variables.
# For Scaleway Mistral API:
SCW_LLM_API_KEY=your_scaleway_api_key_here
# LLM_MODEL_NAME=mistral-nemo-instruct-2407 # Default model used in llm_service.py
# LLM_TEMPERATURE=0.7 # Default temperature
LLM_MAX_MAX_TOKENS=512 # Max tokens for Scaleway API

# For local vLLM or TGI (uncomment and configure):
# LLM_BACKEND=vllm  # vllm ou tgi
# LLM_LOCAL_API_URL=http://localhost:8000  # URL du serveur vLLM ou TGI
# LLM_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.2 # Model name for local server
# LLM_TEMPERATURE=0.7
# LLM_MAX_TOKENS=150
LLM_TIMEOUT_S=30

# VAD Parameters
VAD_THRESHOLD=0.45
VAD_MIN_SILENCE_DURATION_MS=2000 # Seuil pour fin de tour
VAD_SPEECH_PAD_MS=400

# Storage Paths
AUDIO_STORAGE_PATH=./data/audio
FEEDBACK_STORAGE_PATH=./data/feedback

# Coqui TTS specific (Example)
TTS_SPEAKER_ID_NEUTRAL=p225
TTS_SPEAKER_ID_ENCOURAGEMENT=p226
# Or XTTS paths
# TTS_XTTS_MODEL_PATH=...
# TTS_XTTS_CONFIG_PATH=...
# TTS_XTTS_SPEAKER_WAV_ENCOURAGEMENT=./voices/encouragement.wav

SUPABASE_PROJECT_REF=your_project_ref_here
SUPABASE_DB_PASSWORD=your_db_password_here
"""
Agent LiveKit avec Bark TTS - Voix franÃ§aises haute qualitÃ©
Version optimisÃ©e pour coaching vocal avec expressivitÃ© naturelle
"""

import asyncio
import logging
import os
import tempfile
import httpx
import numpy as np
import jwt
import time
from typing import Optional
from dotenv import load_dotenv

# Imports compatibles avec livekit-agents 0.7.2
from livekit import rtc
import webrtcvad

# Charger les variables d'environnement
load_dotenv()

# Configuration du logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("LIVEKIT_AGENT_BARK")

def generate_agent_token(room_name: str, participant_identity: str = "voice_coach_agent_bark") -> str:
    """GÃ©nÃ¨re un token JWT pour l'agent avec les bonnes permissions"""
    api_key = os.getenv("LIVEKIT_API_KEY", "devkey")
    api_secret = os.getenv("LIVEKIT_API_SECRET", "devsecret123456789abcdef0123456789abcdef0123456789abcdef")
    
    now = int(time.time())
    exp = now + (24 * 60 * 60)  # 24 heures
    
    payload = {
        "iss": api_key,
        "sub": participant_identity,
        "iat": now,
        "exp": exp,
        "video": {
            "room": room_name,
            "roomJoin": True,
            "roomList": True,
            "roomRecord": False,
            "roomAdmin": True,  # Permissions d'admin pour l'agent
            "roomCreate": False,
            "canPublish": True,
            "canSubscribe": True,
            "canPublishData": True,
            "canUpdateOwnMetadata": True,
            "agent": True  # Marquer comme agent
        }
    }
    
    token = jwt.encode(payload, api_secret, algorithm="HS256")
    logger.info(f"ğŸ”‘ Token agent Bark gÃ©nÃ©rÃ© pour room: {room_name}")
    return token

class BarkVoiceCoachingAgent:
    """Agent de coaching vocal avec Bark TTS haute qualitÃ©"""
    
    def __init__(self):
        self.room = None
        self.participant = None
        self.audio_frames_processed = 0
        self.transcriptions_made = 0
        self.responses_generated = 0
        
        # Buffer audio pour le VAD
        self.audio_buffer = b''
        self.audio_buffer_sample_rate = 0
        
        # Services
        self.asr_url = "http://asr-service:8001/transcribe"
        # Utilisation de Piper TTS au lieu de Bark/XTTS
        self.tts_url = "http://piper-tts:8000/synthesize"
        
        # Configuration Piper TTS
        self.voice = os.getenv("TTS_VOICE", "default")
        
        # VAD pour dÃ©tection de voix
        self.vad = webrtcvad.Vad(3) # Mode 3: le plus agressif (moins de faux positifs, mais peut manquer de la voix faible)
        
        # RÃ©ponses de coaching personnalisÃ©es
        self.coaching_responses = {
            "encouragement": [
                "Excellent travail ! Votre diction est remarquable.",
                "Bravo ! Votre prononciation s'amÃ©liore considÃ©rablement.",
                "Parfait ! Continuez sur cette lancÃ©e, c'est formidable.",
                "Magnifique ! Votre expressivitÃ© est trÃ¨s naturelle."
            ],
            "correction": [
                "TrÃ¨s bien ! Essayons de travailler un peu plus l'articulation.",
                "C'est bien ! Pouvons-nous rÃ©pÃ©ter en articulant davantage ?",
                "Bonne tentative ! Concentrons-nous sur la fluiditÃ©.",
                "IntÃ©ressant ! Travaillons ensemble l'intonation."
            ],
            "exercices": [
                "Essayons maintenant un exercice de respiration. Inspirez profondÃ©ment.",
                "Parfait ! Maintenant, rÃ©pÃ©tez aprÃ¨s moi : 'Les chaussettes de l'archiduchesse'.",
                "Excellent ! Travaillons la projection vocale. Parlez plus fort.",
                "Formidable ! Concentrons-nous sur le rythme de votre Ã©locution."
            ]
        }
        
        logger.info(f"ğŸ¯ Agent Piper TTS initialisÃ© avec voix: {self.voice}")
    
    async def transcribe_audio(self, audio_data: bytes) -> Optional[str]:
        """Transcription audio via notre service Whisper"""
        try:
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                tmp_file.write(audio_data)
                tmp_file_path = tmp_file.name
            
            async with httpx.AsyncClient() as client:
                with open(tmp_file_path, 'rb') as audio_file:
                    files = {'file': ('audio.wav', audio_file, 'audio/wav')}
                    response = await client.post(
                        self.asr_url,
                        files=files,
                        timeout=30.0
                    )
                
                if response.status_code == 200:
                    result = response.json()
                    transcription = result.get('text', '').strip()
                    logger.info(f"ğŸ“ Transcription: '{transcription}'")
                    return transcription
                else:
                    logger.warning(f"âš ï¸ ASR erreur {response.status_code}")
                    return None
                    
        except Exception as e:
            logger.error(f"âŒ Erreur STT: {e}")
            return None
        finally:
            try:
                os.unlink(tmp_file_path)
            except:
                pass
    
    def analyze_speech_quality(self, transcription: str) -> str:
        """Analyse la qualitÃ© de la parole et choisit le type de rÃ©ponse"""
        if not transcription:
            return "encouragement"
        
        # Analyse simple basÃ©e sur la longueur et la complexitÃ©
        words = transcription.split()
        
        if len(words) >= 5 and len(transcription) > 20:
            return "encouragement"  # Bonne performance
        elif len(words) >= 2:
            return "correction"     # Performance moyenne
        else:
            return "exercices"      # Besoin d'exercices
    
    async def generate_response_audio(self, transcription: str) -> Optional[bytes]:
        """GÃ©nÃ¨re une rÃ©ponse audio avec Bark TTS haute qualitÃ©"""
        try:
            # Analyser la qualitÃ© de la parole
            response_type = self.analyze_speech_quality(transcription)
            
            # Choisir une rÃ©ponse appropriÃ©e
            import random
            responses = self.coaching_responses[response_type]
            response_text = random.choice(responses)
            
            # Si on a une transcription, l'inclure dans la rÃ©ponse
            if transcription and len(transcription.strip()) > 2:
                if response_type == "encouragement":
                    response_text = f"Parfait ! Vous avez dit '{transcription}'. {response_text}"
                elif response_type == "correction":
                    response_text = f"J'ai entendu '{transcription}'. {response_text}"
            
            # PrÃ©parer la requÃªte pour Piper TTS
            piper_payload = {
                'text': response_text,
                'voice': self.voice
            }
            
            logger.info(f"ğŸ¤ RequÃªte Piper TTS: {response_text[:50]}...")

            async with httpx.AsyncClient() as client:
                try:
                    response = await client.post(
                        self.tts_url,
                        json=piper_payload,
                        timeout=30.0
                    )
                    response.raise_for_status()
                    
                    audio_data = response.content
                    self.responses_generated += 1
                    logger.info(f"ğŸ—£ï¸ Piper TTS gÃ©nÃ©rÃ©: {len(audio_data)} octets")
                    return audio_data
                except httpx.HTTPStatusError as exc:
                    logger.warning(f"âš ï¸ Piper TTS erreur HTTP {exc.response.status_code}: {exc.response.text}")
                    return None
                except httpx.RequestError as exc:
                    logger.error(f"âŒ Erreur requÃªte Piper TTS: {exc}")
                    return None
                    
        except Exception as e:
            logger.error(f"âŒ Erreur inattendue dans generate_response_audio: {e}")
            return None
    
    async def on_audio_frame(self, frame: rtc.AudioFrame):
        """Traite les frames audio reÃ§ues, en utilisant VAD pour dÃ©tecter la parole.
        Accumule les frames pour former des chunks de 30ms pour le VAD."""
        try:
            self.audio_frames_processed += 1
            logger.debug(f"ğŸ”„ Traitement de la frame audio #{self.audio_frames_processed} (sample_rate: {frame.sample_rate}, num_channels: {frame.num_channels}, taille_donnees: {len(frame.data)} octets)")
            
            # VÃ©rifier les propriÃ©tÃ©s de la frame
            if frame.num_channels != 1 or frame.sample_rate not in [8000, 16000, 32000, 48000]:
                logger.warning(f"âš ï¸ Frame audio inattendue: {frame.sample_rate} Hz, {frame.num_channels} canaux. Attendu: 1 canal, 8/16/32/48 kHz. IgnorÃ©.")
                return

            # LiveKit fournit dÃ©jÃ  du PCM 16-bit, donc frame.data est directement utilisable
            audio_data_bytes = frame.data
            
            # VÃ©rifier la taille des donnÃ©es brutes
            if not audio_data_bytes:
                logger.debug("â„¹ï¸ Frame audio reÃ§ue mais vide, ignorÃ©e.")
                return
            
            # Mettre Ã  jour le sample_rate du buffer si c'est la premiÃ¨re frame ou si le sample_rate change
            if self.audio_buffer_sample_rate == 0:
                self.audio_buffer_sample_rate = frame.sample_rate
            elif self.audio_buffer_sample_rate != frame.sample_rate:
                logger.warning(f"âš ï¸ Changement de sample_rate dÃ©tectÃ© ({self.audio_buffer_sample_rate} -> {frame.sample_rate}). RÃ©initialisation du buffer audio.")
                self.audio_buffer = b''
                self.audio_buffer_sample_rate = frame.sample_rate

            # Ajouter les nouvelles donnÃ©es au buffer
            self.audio_buffer += audio_data_bytes
            
            # Le VAD de webrtcvad fonctionne avec des frames de 10, 20 ou 30 ms.
            # Nous allons utiliser 30ms pour la dÃ©tection de voix.
            # Calculer la taille d'une frame VAD en octets (sample_rate * durÃ©e_ms / 1000 * bytes_per_sample)
            # 16-bit PCM = 2 bytes par Ã©chantillon
            frame_duration_ms = 30
            bytes_per_frame_vad = int(self.audio_buffer_sample_rate * (frame_duration_ms / 1000.0) * 2)

            # Traiter les donnÃ©es par blocs compatibles VAD tant que le buffer est assez grand
            while len(self.audio_buffer) >= bytes_per_frame_vad:
                chunk = self.audio_buffer[:bytes_per_frame_vad]
                self.audio_buffer = self.audio_buffer[bytes_per_frame_vad:] # Retirer le chunk traitÃ© du buffer

                # Calculer le niveau audio RMS pour le debug
                audio_array = np.frombuffer(chunk, dtype=np.int16)
                rms = np.sqrt(np.mean(audio_array**2))
                max_val = np.max(np.abs(audio_array)) if len(audio_array) > 0 else 0
                
                # Log du niveau audio toutes les 10 frames
                if self.audio_frames_processed % 10 == 0:
                    logger.debug(f"ğŸ“Š Niveau audio - RMS: {rms:.2f}, Max: {max_val}, Taille chunk: {len(chunk)} octets")

                if self.vad.is_speech(chunk, self.audio_buffer_sample_rate):
                    logger.debug(f"ğŸ—£ï¸ Voix dÃ©tectÃ©e dans un chunk de {frame_duration_ms}ms (RMS: {rms:.2f}). Envoi Ã  l'ASR.")
                    
                    # Convertir en WAV pour le STT
                    import wave
                    import io
                    
                    wav_buffer = io.BytesIO()
                    with wave.open(wav_buffer, 'wb') as wav_file:
                        wav_file.setnchannels(frame.num_channels)
                        wav_file.setsampwidth(2)  # 16-bit
                        wav_file.setframerate(self.audio_buffer_sample_rate)
                        wav_file.writeframes(chunk) # Utiliser le chunk dÃ©tectÃ© comme voix
                    
                    wav_data = wav_buffer.getvalue()
                    logger.debug(f"ğŸ“Š DonnÃ©es WAV prÃ©parÃ©es pour ASR (taille: {len(wav_data)} octets).")
                    
                    # Transcription
                    transcription = await self.transcribe_audio(wav_data)
                    
                    if transcription and len(transcription.strip()) > 2:
                        self.transcriptions_made += 1
                        logger.info(f"âœ… Transcription non vide: '{transcription}'")
                        
                        # GÃ©nÃ©rer une rÃ©ponse audio avec Bark
                        response_audio = await self.generate_response_audio(transcription)
                        
                        if response_audio and self.room:
                            logger.info(f"ğŸ”Š Envoi de la rÃ©ponse audio XTTS (taille: {len(response_audio)} octets).")
                            audio_source = rtc.AudioSource(sample_rate=16000, num_channels=1) # XTTS sort souvent en 24kHz, un rÃ©Ã©chantillonnage pourrait Ãªtre nÃ©cessaire ici ou cÃ´tÃ© serveur XTTS
                            track = rtc.LocalAudioTrack.create_audio_track("xtts_coaching_response", audio_source)
                            
                            await self.room.local_participant.publish_track(track, rtc.TrackPublishOptions())
                            
                            # S'assurer que response_audio est bien en PCM 16kHz mono pour LiveKit
                            # Un rÃ©Ã©chantillonnage et une conversion de format pourraient Ãªtre nÃ©cessaires ici
                            # si le service XTTS ne le fait pas. Pour l'instant, on suppose qu'il est correct.
                            await audio_source.capture_frame(rtc.AudioFrame(
                                data=response_audio,
                                sample_rate=16000,
                                num_channels=1,
                                samples_per_channel=len(response_audio) // 2
                            ))
                            logger.info("âœ… RÃ©ponse audio envoyÃ©e.")
                        else:
                            logger.warning("âš ï¸ Aucune rÃ©ponse audio gÃ©nÃ©rÃ©e ou room non disponible.")
                    else:
                        logger.info("ğŸ“ Transcription vide ou trop courte, pas de rÃ©ponse gÃ©nÃ©rÃ©e.")
                else:
                    logger.debug("ğŸ”‡ Silence dÃ©tectÃ© dans le chunk audio, ignorÃ©.")
            
        except Exception as e:
            logger.error(f"âŒ Erreur traitement audio dans on_audio_frame: {e}")
    
    async def connect_to_room(self, room_url: str, token: str):
        """Se connecte Ã  la room LiveKit"""
        try:
            self.room = rtc.Room()
            
            # Ã‰vÃ©nements de la room
            @self.room.on("participant_connected")
            def on_participant_connected(participant: rtc.RemoteParticipant):
                logger.info(f"ğŸ‘¤ Participant connectÃ©: {participant.identity}")

            # Nouvelle fonction pour traiter le flux audio d'une piste distante
            async def _process_remote_audio_stream(track: rtc.RemoteAudioTrack, participant_identity: str): # Ajout de participant_identity
                logger.info(f"ğŸ§ DÃ©marrage du traitement du flux audio pour la piste: {track.sid} de {participant_identity}") # Utilisation de participant_identity
                try:
                    # Utiliser track.stream() pour obtenir un AsyncIterable[AudioFrame]
                    # CrÃ©er un AudioStream Ã  partir de la piste audio distante
                    audio_stream = rtc.AudioStream(track) # MODIFICATION ICI
                    # ItÃ©rer sur le flux audio pour obtenir les frames
                    async for frame in audio_stream: # MODIFICATION ICI
                        # AccÃ©der Ã  la frame audio via event.frame
                        await self.on_audio_frame(frame.frame)
                except Exception as e:
                    logger.error(f"âŒ Erreur pendant le traitement du flux audio pour la piste {track.sid} de {participant_identity}: {e}")
                finally:
                    logger.info(f"ğŸ Fin du traitement du flux audio pour la piste: {track.sid} de {participant_identity}")

            @self.room.on("track_subscribed")
            def on_track_subscribed(track: rtc.Track,
                                    publication: rtc.TrackPublication,
                                    participant: rtc.RemoteParticipant):
                logger.info(f"ğŸµ Track souscrit: {track.kind} (SID: {track.sid}) de {participant.identity}")
                if track.kind == rtc.TrackKind.KIND_AUDIO and isinstance(track, rtc.RemoteAudioTrack):
                    if participant.identity != self.room.local_participant.identity:
                        logger.info(f"ğŸ¤ Piste audio distante dÃ©tectÃ©e de {participant.identity}, SID: {track.sid}. Lancement du traitement du flux.")
                        # Lancer une tÃ¢che asyncio pour traiter le flux de cette piste
                        asyncio.create_task(_process_remote_audio_stream(track, participant.identity)) # Passer participant.identity
                    else:
                        logger.info(f"ğŸ¤ Piste audio locale (de l'agent) dÃ©tectÃ©e de {participant.identity}, SID: {track.sid}, ignorÃ©e pour la souscription de frame.")
                else:
                    logger.info(f"â„¹ï¸ Piste non-audio ou non-distante souscrite: {track.kind} (SID: {track.sid}) de {participant.identity}")
            
            # Se connecter
            await self.room.connect(room_url, token)
            logger.info(f"âœ… ConnectÃ© Ã  la room: {self.room.name}")
            
            # Message d'accueil avec Piper TTS
            welcome_message = "Bonjour ! Je suis votre coach vocal IA. Commencez Ã  parler pour que je puisse vous accompagner."
            welcome_audio = await self.generate_response_audio(welcome_message)
            
            if welcome_audio:
                # Publier le message d'accueil
                audio_source = rtc.AudioSource(sample_rate=16000, num_channels=1) # Idem, vÃ©rifier le format de sortie XTTS
                track = rtc.LocalAudioTrack.create_audio_track("xtts_welcome", audio_source)
                await self.room.local_participant.publish_track(track, rtc.TrackPublishOptions())
                
                await audio_source.capture_frame(rtc.AudioFrame(
                    data=welcome_audio,
                    sample_rate=16000,
                    num_channels=1,
                    samples_per_channel=len(welcome_audio) // 2
                ))
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur connexion room: {e}")
            return False

async def main():
    """Point d'entrÃ©e principal"""
    # Lire les informations de connexion depuis les variables d'environnement
    room_name = os.getenv("ROOM_NAME")
    participant_identity = os.getenv("PARTICIPANT_IDENTITY", "voice_coach_agent_bark")
    livekit_url = os.getenv("LIVEKIT_URL", "ws://livekit-server:7880")

    if not room_name:
        logger.error("âŒ Variable d'environnement ROOM_NAME doit Ãªtre dÃ©finie.")
        return

    logger.info(f"ğŸš€ Lancement de l'agent Bark pour room: {room_name}")
    logger.info(f"ğŸ¤ Voix configurÃ©e: {os.getenv('TTS_VOICE', 'v2/fr_speaker_1')}")
    
    # GÃ©nÃ©rer le token pour l'agent
    agent_token = generate_agent_token(room_name, participant_identity)
    logger.info(f"ğŸ”‘ Token agent: {agent_token[:50]}...")
    
    # CrÃ©er et dÃ©marrer l'agent
    agent = BarkVoiceCoachingAgent()
    
    success = await agent.connect_to_room(livekit_url, agent_token)
    
    if success:
        logger.info("âœ… Agent Bark dÃ©marrÃ© avec succÃ¨s")
        
        # Maintenir la connexion
        try:
            while True:
                await asyncio.sleep(5)
                
                # Log des statistiques pÃ©riodiquement
                if agent.audio_frames_processed % 100 == 0 and agent.audio_frames_processed > 0:
                    logger.info(f"ğŸ“Š Stats - Frames: {agent.audio_frames_processed}, Transcriptions: {agent.transcriptions_made}, RÃ©ponses Bark: {agent.responses_generated}")
                    
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ ArrÃªt de l'agent Bark demandÃ©")
        except Exception as e:
            logger.error(f"âŒ Erreur dans la boucle principale: {e}")
        finally:
            if agent.room:
                await agent.room.disconnect()
            logger.info("ğŸ”Œ Agent Bark dÃ©connectÃ©")
    else:
        logger.error("âŒ Impossible de dÃ©marrer l'agent Bark")

if __name__ == "__main__":
    asyncio.run(main())

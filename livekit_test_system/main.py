#!/usr/bin/env python3
"""
Syst√®me de test LiveKit pour le coaching vocal interactif
Script principal pour lancer les tests de streaming audio
"""

import asyncio
import argparse
import signal
import sys
import json
from pathlib import Path
from typing import Dict, Any, Optional
import os

from pipeline_logger import PipelineLogger, metrics_collector
from test_orchestrator import LiveKitTestOrchestrator

# Configuration par d√©faut
DEFAULT_CONFIG = {
    "livekit_url": "ws://localhost:7880",
    "api_key": "devkey",
    "api_secret": "secret",
    "room_name": "test_coaching_vocal",
    "temp_dir": "./temp_audio_test"
}

class LiveKitTestRunner:
    """
    Runner principal pour les tests LiveKit
    G√®re l'ex√©cution des diff√©rents sc√©narios de test
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = PipelineLogger("TEST_RUNNER")
        self.orchestrator: Optional[LiveKitTestOrchestrator] = None
        self.shutdown_requested = False
        
        # Configurer la gestion des signaux
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        self.logger.info("üéØ Test Runner LiveKit initialis√©")
    
    def _signal_handler(self, signum, frame):
        """Gestionnaire pour l'arr√™t propre"""
        self.logger.warning(f"üõë Signal re√ßu: {signum}")
        self.shutdown_requested = True
        
        if self.orchestrator:
            self.orchestrator.stop_test()
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """
        Ex√©cute tous les tests disponibles
        
        Returns:
            R√©sultats consolid√©s de tous les tests
        """
        self.logger.info("üöÄ D√©marrage de la suite compl√®te de tests")
        
        all_results = {
            'test_suite': 'complete',
            'start_time': asyncio.get_event_loop().time(),
            'tests': {},
            'summary': {}
        }
        
        try:
            # Initialiser l'orchestrateur
            self.orchestrator = LiveKitTestOrchestrator(self.config)
            await self.orchestrator.initialize_components()
            
            # Connecter les clients
            connected = await self.orchestrator.connect_clients()
            if not connected:
                self.logger.error("‚ùå Impossible de connecter les clients")
                return all_results
            
            # Test 1: Test de base
            if not self.shutdown_requested:
                self.logger.info("\n" + "="*50)
                self.logger.info("üß™ TEST 1: Test de base")
                self.logger.info("="*50)
                
                try:
                    result = await self.orchestrator.run_basic_test(duration_seconds=30)
                    all_results['tests']['basic_test'] = result
                    self.logger.success("‚úÖ Test de base termin√©")
                except Exception as e:
                    self.logger.error(f"üí• √âchec test de base: {e}")
                    all_results['tests']['basic_test'] = {'error': str(e)}
            
            # Pause entre les tests
            if not self.shutdown_requested:
                self.logger.info("‚è≥ Pause de 5 secondes entre les tests...")
                await asyncio.sleep(5)
            
            # Test 2: Test de stress
            if not self.shutdown_requested:
                self.logger.info("\n" + "="*50)
                self.logger.info("üî• TEST 2: Test de stress")
                self.logger.info("="*50)
                
                try:
                    result = await self.orchestrator.run_stress_test(
                        packets_count=30,
                        interval_ms=800
                    )
                    all_results['tests']['stress_test'] = result
                    self.logger.success("‚úÖ Test de stress termin√©")
                except Exception as e:
                    self.logger.error(f"üí• √âchec test de stress: {e}")
                    all_results['tests']['stress_test'] = {'error': str(e)}
            
            # Pause entre les tests
            if not self.shutdown_requested:
                self.logger.info("‚è≥ Pause de 5 secondes entre les tests...")
                await asyncio.sleep(5)
            
            # Test 3: Test de latence
            if not self.shutdown_requested:
                self.logger.info("\n" + "="*50)
                self.logger.info("‚ö° TEST 3: Test de latence")
                self.logger.info("="*50)
                
                try:
                    result = await self.orchestrator.run_latency_test(quick_packets=15)
                    all_results['tests']['latency_test'] = result
                    self.logger.success("‚úÖ Test de latence termin√©")
                except Exception as e:
                    self.logger.error(f"üí• √âchec test de latence: {e}")
                    all_results['tests']['latency_test'] = {'error': str(e)}
            
            # Compiler le r√©sum√©
            all_results['summary'] = self._compile_summary(all_results['tests'])
            all_results['end_time'] = asyncio.get_event_loop().time()
            all_results['total_duration'] = all_results['end_time'] - all_results['start_time']
            
            return all_results
            
        except Exception as e:
            self.logger.error(f"üí• Erreur fatale dans la suite de tests: {e}")
            all_results['fatal_error'] = str(e)
            return all_results
        
        finally:
            # Nettoyage
            if self.orchestrator:
                await self.orchestrator.disconnect_all()
                self.orchestrator.print_final_summary()
    
    async def run_single_test(self, test_name: str, **kwargs) -> Dict[str, Any]:
        """
        Ex√©cute un test sp√©cifique
        
        Args:
            test_name: Nom du test ('basic', 'stress', 'latency')
            **kwargs: Param√®tres sp√©cifiques au test
        
        Returns:
            R√©sultats du test
        """
        self.logger.info(f"üéØ Ex√©cution du test: {test_name}")
        
        try:
            # Initialiser l'orchestrateur
            self.orchestrator = LiveKitTestOrchestrator(self.config)
            await self.orchestrator.initialize_components()
            
            # Connecter les clients
            connected = await self.orchestrator.connect_clients()
            if not connected:
                self.logger.error("‚ùå Impossible de connecter les clients")
                return {'error': 'connection_failed'}
            
            # Ex√©cuter le test demand√©
            if test_name == 'basic':
                duration = kwargs.get('duration', 30)
                result = await self.orchestrator.run_basic_test(duration_seconds=duration)
            
            elif test_name == 'stress':
                packets = kwargs.get('packets', 50)
                interval = kwargs.get('interval', 500)
                result = await self.orchestrator.run_stress_test(
                    packets_count=packets,
                    interval_ms=interval
                )
            
            elif test_name == 'latency':
                packets = kwargs.get('packets', 20)
                result = await self.orchestrator.run_latency_test(quick_packets=packets)
            
            else:
                self.logger.error(f"‚ùå Test inconnu: {test_name}")
                return {'error': f'unknown_test: {test_name}'}
            
            self.logger.success(f"‚úÖ Test {test_name} termin√© avec succ√®s")
            return result
            
        except Exception as e:
            self.logger.error(f"üí• Erreur dans le test {test_name}: {e}")
            return {'error': str(e)}
        
        finally:
            # Nettoyage
            if self.orchestrator:
                await self.orchestrator.disconnect_all()
                self.orchestrator.print_final_summary()
    
    def _compile_summary(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """Compile un r√©sum√© des r√©sultats de tests"""
        summary = {
            'tests_executed': len(test_results),
            'tests_successful': 0,
            'tests_failed': 0,
            'total_packets_sent': 0,
            'total_packets_received': 0,
            'overall_packet_loss': 0.0,
            'average_latency_ms': 0.0,
            'total_errors': 0
        }
        
        latency_measurements = []
        
        for test_name, result in test_results.items():
            if 'error' in result:
                summary['tests_failed'] += 1
                continue
            
            summary['tests_successful'] += 1
            summary['total_packets_sent'] += result.get('packets_sent', 0)
            summary['total_packets_received'] += result.get('packets_received', 0)
            summary['total_errors'] += result.get('errors_count', 0)
            
            # Collecter les mesures de latence
            latency_stats = result.get('latency_stats', {})
            if 'avg_ms' in latency_stats:
                latency_measurements.append(latency_stats['avg_ms'])
        
        # Calculer les moyennes globales
        if summary['total_packets_sent'] > 0:
            summary['overall_packet_loss'] = max(0, 
                (summary['total_packets_sent'] - summary['total_packets_received']) / 
                summary['total_packets_sent']
            )
        
        if latency_measurements:
            summary['average_latency_ms'] = sum(latency_measurements) / len(latency_measurements)
        
        return summary
    
    def save_results(self, results: Dict[str, Any], output_file: Optional[str] = None):
        """Sauvegarde les r√©sultats dans un fichier JSON"""
        if not output_file:
            timestamp = int(asyncio.get_event_loop().time())
            output_file = f"livekit_test_results_{timestamp}.json"
        
        output_path = Path(output_file)
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            self.logger.success(f"üìÑ R√©sultats sauvegard√©s: {output_path}")
            
        except Exception as e:
            self.logger.error(f"üí• Erreur sauvegarde: {e}")


def load_config(config_file: Optional[str] = None) -> Dict[str, Any]:
    """Charge la configuration depuis un fichier ou utilise les valeurs par d√©faut"""
    config = DEFAULT_CONFIG.copy()
    
    # Charger depuis un fichier si sp√©cifi√©
    if config_file and Path(config_file).exists():
        try:
            with open(config_file, 'r') as f:
                file_config = json.load(f)
                config.update(file_config)
            print(f"‚úÖ Configuration charg√©e depuis: {config_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur chargement config: {e}, utilisation des valeurs par d√©faut")
    
    # Surcharger avec les variables d'environnement
    env_mappings = {
        'LIVEKIT_URL': 'livekit_url',
        'LIVEKIT_API_KEY': 'api_key',
        'LIVEKIT_API_SECRET': 'api_secret',
        'LIVEKIT_ROOM': 'room_name'
    }
    
    for env_var, config_key in env_mappings.items():
        if os.getenv(env_var):
            config[config_key] = os.getenv(env_var)
    
    return config


async def main():
    """Fonction principale"""
    parser = argparse.ArgumentParser(
        description="Syst√®me de test LiveKit pour coaching vocal",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python main.py --all                    # Ex√©cute tous les tests
  python main.py --test basic --duration 60  # Test de base de 60s
  python main.py --test stress --packets 100  # Test de stress avec 100 paquets
  python main.py --test latency --packets 30  # Test de latence avec 30 paquets
  python main.py --config my_config.json     # Utilise une config personnalis√©e
        """
    )
    
    parser.add_argument('--config', '-c', 
                       help='Fichier de configuration JSON')
    parser.add_argument('--all', action='store_true',
                       help='Ex√©cuter tous les tests')
    parser.add_argument('--test', choices=['basic', 'stress', 'latency'],
                       help='Ex√©cuter un test sp√©cifique')
    parser.add_argument('--duration', type=int, default=30,
                       help='Dur√©e du test de base en secondes')
    parser.add_argument('--packets', type=int, default=50,
                       help='Nombre de paquets pour les tests stress/latence')
    parser.add_argument('--interval', type=int, default=500,
                       help='Intervalle en ms pour le test de stress')
    parser.add_argument('--output', '-o',
                       help='Fichier de sortie pour les r√©sultats JSON')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Mode verbeux')
    
    args = parser.parse_args()
    
    # Charger la configuration
    config = load_config(args.config)
    
    # Cr√©er le r√©pertoire temporaire
    temp_dir = Path(config['temp_dir'])
    temp_dir.mkdir(exist_ok=True)
    
    # Afficher la configuration
    print("\n" + "="*60)
    print("üé≠ SYST√àME DE TEST LIVEKIT - COACHING VOCAL")
    print("="*60)
    print(f"üåê LiveKit URL: {config['livekit_url']}")
    print(f"üè† Room: {config['room_name']}")
    print(f"üìÅ R√©pertoire temp: {config['temp_dir']}")
    print("="*60 + "\n")
    
    # Cr√©er le runner
    runner = LiveKitTestRunner(config)
    
    try:
        # Ex√©cuter les tests selon les arguments
        if args.all:
            results = await runner.run_all_tests()
        elif args.test:
            test_kwargs = {}
            if args.test == 'basic':
                test_kwargs['duration'] = args.duration
            elif args.test == 'stress':
                test_kwargs['packets'] = args.packets
                test_kwargs['interval'] = args.interval
            elif args.test == 'latency':
                test_kwargs['packets'] = args.packets
            
            results = await runner.run_single_test(args.test, **test_kwargs)
        else:
            print("‚ùå Veuillez sp√©cifier --all ou --test <nom_test>")
            parser.print_help()
            return 1
        
        # Sauvegarder les r√©sultats
        if args.output or args.all:
            runner.save_results(results, args.output)
        
        # Afficher un r√©sum√© final
        if 'summary' in results:
            summary = results['summary']
            print(f"\nüìä R√âSUM√â FINAL:")
            print(f"  ‚úÖ Tests r√©ussis: {summary['tests_successful']}")
            print(f"  ‚ùå Tests √©chou√©s: {summary['tests_failed']}")
            print(f"  üì§ Paquets envoy√©s: {summary['total_packets_sent']}")
            print(f"  üì• Paquets re√ßus: {summary['total_packets_received']}")
            print(f"  üìâ Taux de perte: {summary['overall_packet_loss']:.2%}")
            print(f"  ‚ö° Latence moyenne: {summary['average_latency_ms']:.2f}ms")
            print(f"  ‚ùå Erreurs totales: {summary['total_errors']}")
        
        return 0
        
    except KeyboardInterrupt:
        print("\nüõë Test interrompu par l'utilisateur")
        return 130
    except Exception as e:
        print(f"\nüí• Erreur fatale: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))
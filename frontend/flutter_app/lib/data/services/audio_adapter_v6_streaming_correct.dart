import 'dart:async';
import 'dart:typed_data';
import 'package:just_audio/just_audio.dart';
import '../../core/utils/logger_service.dart';
import '../../data/models/session_model.dart';
import '../../src/services/livekit_service.dart';
import 'audio_format_detector_v2.dart';

/// AudioAdapter V6 avec StreamAudioSource pour flux continu
/// Bas√© sur la documentation officielle just_audio
class AudioAdapterV6StreamingCorrect {
  static const String _tag = 'AudioAdapterV6StreamingCorrect';
  
  final LiveKitService _liveKitService;
  final AudioFormatDetectorV2 _formatDetector = AudioFormatDetectorV2();
  
  // Lecteur audio
  late AudioPlayer _audioPlayer;
  
  // Stream controller pour les donn√©es audio
  late StreamController<List<int>> _audioStreamController;
  late MyCustomAudioSource _customAudioSource;
  
  // Buffer pour accumulation
  final List<int> _audioBuffer = [];
  static const int _sampleRate = 48000;
  static const int _channels = 1;
  static const int _bytesPerSample = 2; // PCM16
  
  // √âtat
  bool _isInitialized = false;
  bool _isConnected = false;
  bool _isRecording = false;
  bool _isPlaying = false;
  
  // Callbacks
  Function(String)? onTextReceived;
  Function(String)? onError;
  
  AudioAdapterV6StreamingCorrect(this._liveKitService);
  
  /// Initialise l'adaptateur V6 avec StreamAudioSource
  Future<bool> initialize() async {
    try {
      logger.i(_tag, 'üéµ [V6_STREAMING] ===== INITIALISATION ADAPTATEUR V6 STREAMING =====');
      
      // Cr√©er le lecteur audio
      _audioPlayer = AudioPlayer();
      
      // Cr√©er le stream controller
      _audioStreamController = StreamController<List<int>>.broadcast();
      
      // Cr√©er la source audio personnalis√©e
      _customAudioSource = MyCustomAudioSource(_audioStreamController.stream);
      
      // Configurer le lecteur avec la source streaming
      await _audioPlayer.setAudioSource(_customAudioSource);
      
      // √âcouter les changements d'√©tat
      _audioPlayer.playerStateStream.listen((state) {
        logger.v(_tag, 'üéµ [V6_STREAMING] √âtat chang√©: playing=${state.playing}, processingState=${state.processingState}');
        
        switch (state.processingState) {
          case ProcessingState.idle:
            logger.v(_tag, '‚è∏Ô∏è [V6_STREAMING] √âtat: Idle');
            break;
          case ProcessingState.loading:
            logger.v(_tag, '‚è≥ [V6_STREAMING] √âtat: Loading');
            break;
          case ProcessingState.buffering:
            logger.v(_tag, 'üîÑ [V6_STREAMING] √âtat: Buffering');
            break;
          case ProcessingState.ready:
            logger.v(_tag, '‚úÖ [V6_STREAMING] √âtat: Ready');
            break;
          case ProcessingState.completed:
            logger.v(_tag, 'üèÅ [V6_STREAMING] √âtat: Completed');
            break;
        }
      });
      
      // √âcouter les erreurs
      _audioPlayer.errorStream.listen((error) {
        logger.e(_tag, '‚ùå [V6_STREAMING] Erreur audio: ${error.message}');
        onError?.call('Erreur audio: ${error.message}');
      });
      
      _isInitialized = true;
      logger.i(_tag, '‚úÖ [V6_STREAMING] Adaptateur V6 initialis√© avec succ√®s');
      logger.i(_tag, 'üéµ [V6_STREAMING] ===== FIN INITIALISATION ADAPTATEUR V6 =====');
      
      return true;
    } catch (e) {
      logger.e(_tag, '‚ùå [V6_STREAMING] Erreur lors de l\'initialisation: $e');
      return false;
    }
  }
  
  /// Connecte √† LiveKit
  Future<bool> connectToLiveKit(SessionModel session) async {
    try {
      logger.i(_tag, 'üîó [V6_STREAMING] Connexion √† LiveKit...');
      
      // Connecter via LiveKitService
      final success = await _liveKitService.connectWithToken(
        session.livekitUrl,
        session.token,
        roomName: session.roomName,
      );
      
      if (success) {
        // Configurer les callbacks pour recevoir l'audio
        _liveKitService.onDataReceived = _handleAudioData;
        
        _isConnected = true;
        logger.i(_tag, '‚úÖ [V6_STREAMING] Connexion LiveKit r√©ussie');
      } else {
        logger.e(_tag, '‚ùå [V6_STREAMING] √âchec connexion LiveKit');
      }
      
      return success;
    } catch (e) {
      logger.e(_tag, '‚ùå [V6_STREAMING] Erreur connexion LiveKit: $e');
      return false;
    }
  }
  
  /// D√©marre l'enregistrement
  Future<bool> startRecording() async {
    try {
      logger.i(_tag, 'üé§ [V6_STREAMING] D√©marrage de l\'enregistrement...');
      
      if (!_isConnected) {
        logger.e(_tag, '‚ùå [V6_STREAMING] Pas connect√© √† LiveKit');
        return false;
      }
      
      // Activer la r√©ception audio dans LiveKit
      _liveKitService.startAcceptingAudioData();
      
      // Publier notre audio
      await _liveKitService.publishMyAudio();
      
      _isRecording = true;
      logger.i(_tag, '‚úÖ [V6_STREAMING] Enregistrement d√©marr√©');
      
      return true;
    } catch (e) {
      logger.e(_tag, '‚ùå [V6_STREAMING] Erreur d√©marrage enregistrement: $e');
      return false;
    }
  }
  
  /// Arr√™te l'enregistrement
  Future<bool> stopRecording() async {
    try {
      logger.i(_tag, 'üõë [V6_STREAMING] Arr√™t de l\'enregistrement...');
      
      _isRecording = false;
      
      // Arr√™ter la lecture audio
      await _audioPlayer.stop();
      
      // Fermer le stream
      await _audioStreamController.close();
      
      logger.i(_tag, '‚úÖ [V6_STREAMING] Enregistrement arr√™t√©');
      return true;
    } catch (e) {
      logger.e(_tag, '‚ùå [V6_STREAMING] Erreur arr√™t enregistrement: $e');
      return false;
    }
  }
  
  /// G√®re les donn√©es audio re√ßues
  void _handleAudioData(Uint8List audioData) {
    try {
      logger.v(_tag, 'üì• [V6_STREAMING] Donn√©es audio re√ßues: ${audioData.length} octets');
      
      // Analyser le format avec la bonne m√©thode
      final formatResult = AudioFormatDetectorV2.processAudioData(audioData);
      
      if (formatResult.format == AudioFormatV2.silence) {
        logger.w(_tag, '‚ùå [V6_STREAMING] Donn√©es rejet√©es: Silence d√©tect√©');
        return;
      }
      
      if (formatResult.format == AudioFormatV2.pcm16) {
        logger.v(_tag, '‚úÖ [V6_STREAMING] Format PCM16 valid√©, qualit√©: ${formatResult.quality?.toStringAsFixed(3) ?? "N/A"}');
        
        // Ajouter directement au stream (pas de conversion WAV n√©cessaire)
        _audioBuffer.addAll(audioData);
        
        // Envoyer les donn√©es au stream audio
        if (!_audioStreamController.isClosed) {
          _audioStreamController.add(List<int>.from(audioData));
          
          // D√©marrer la lecture si pas encore d√©marr√©e
          if (!_isPlaying) {
            _startPlayback();
          }
        }
      }
    } catch (e) {
      logger.e(_tag, '‚ùå [V6_STREAMING] Erreur traitement audio: $e');
    }
  }
  
  /// D√©marre la lecture audio
  void _startPlayback() async {
    try {
      if (_isPlaying) return;
      
      logger.i(_tag, 'üîä [V6_STREAMING] D√©marrage de la lecture streaming...');
      _isPlaying = true;
      
      // D√©marrer la lecture
      await _audioPlayer.play();
      
      logger.i(_tag, '‚úÖ [V6_STREAMING] Lecture streaming d√©marr√©e');
    } catch (e) {
      logger.e(_tag, '‚ùå [V6_STREAMING] Erreur d√©marrage lecture: $e');
      _isPlaying = false;
    }
  }
  
  
  /// Nettoie les ressources
  Future<void> dispose() async {
    try {
      logger.i(_tag, 'üßπ [V6_STREAMING] Nettoyage des ressources...');
      
      _isRecording = false;
      _isPlaying = false;
      
      await _audioPlayer.dispose();
      
      if (!_audioStreamController.isClosed) {
        await _audioStreamController.close();
      }
      
      logger.i(_tag, '‚úÖ [V6_STREAMING] Ressources nettoy√©es');
    } catch (e) {
      logger.e(_tag, '‚ùå [V6_STREAMING] Erreur nettoyage: $e');
    }
  }
  
  // Getters
  bool get isInitialized => _isInitialized;
  bool get isConnected => _isConnected;
  bool get isRecording => _isRecording;
}

/// Source audio personnalis√©e bas√©e sur la documentation just_audio
class MyCustomAudioSource extends StreamAudioSource {
  final Stream<List<int>> _audioStream;
  final List<int> _buffer = [];
  bool _streamCompleted = false;
  
  MyCustomAudioSource(this._audioStream) {
    // √âcouter le stream et accumuler dans le buffer
    _audioStream.listen(
      (chunk) {
        _buffer.addAll(chunk);
      },
      onDone: () {
        _streamCompleted = true;
      },
      onError: (error) {
        _streamCompleted = true;
      },
    );
  }
  
  @override
  Future<StreamAudioResponse> request([int? start, int? end]) async {
    start ??= 0;
    end ??= _buffer.length;
    
    // Attendre qu'il y ait des donn√©es si n√©cessaire
    int waitCount = 0;
    while (_buffer.length < end && _buffer.length < start + 4096 && !_streamCompleted && waitCount < 100) {
      await Future.delayed(Duration(milliseconds: 10));
      waitCount++;
    }
    
    if (start >= _buffer.length) {
      // Pas encore de donn√©es
      return StreamAudioResponse(
        sourceLength: _streamCompleted ? _buffer.length : null,
        contentLength: 0,
        offset: start,
        stream: Stream.empty(),
        contentType: 'audio/wav', // Changer en WAV pour compatibilit√©
      );
    }
    
    final actualEnd = end > _buffer.length ? _buffer.length : end;
    final chunk = _buffer.sublist(start, actualEnd);
    
    return StreamAudioResponse(
      sourceLength: _streamCompleted ? _buffer.length : null,
      contentLength: chunk.length,
      offset: start,
      stream: Stream.value(chunk),
      contentType: 'audio/wav', // Changer en WAV pour compatibilit√©
    );
  }
}